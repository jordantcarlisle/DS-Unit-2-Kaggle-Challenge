{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3XH_XLsy_Bn"
   },
   "source": [
    "_Lambda School Data Science, Unit 2_\n",
    " \n",
    "# Sprint Challenge: Predict Steph Curry's shots ðŸ€\n",
    "\n",
    "For your Sprint Challenge, you'll use a dataset with all Steph Curry's NBA field goal attempts. (Regular season and playoff games, from October 28, 2009, through June 5, 2019.) \n",
    "\n",
    "You'll predict whether each shot was made, using information about the shot and the game. This is hard to predict! Try to get above 60% accuracy. The dataset was collected with the [nba_api](https://github.com/swar/nba_api) Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nw3CL7TE7tNq"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Install packages in Colab\n",
    "    !pip install category_encoders==2.*\n",
    "    !pip install pandas-profiling==2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Nm24pCHy_Bo"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "import pandas as pd\n",
    "url = 'https://drive.google.com/uc?export=download&id=1fL7KPyxgGYfQDsuJoBWHIWwCAf-HTFpX'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Check data shape\n",
    "assert df.shape == (13958, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8BvDKLFy_Bq"
   },
   "source": [
    "To demonstrate mastery on your Sprint Challenge, do all the required, numbered instructions in this notebook.\n",
    "\n",
    "To earn a score of \"3\", also do all the stretch goals.\n",
    "\n",
    "You are permitted and encouraged to do as much data exploration as you want.\n",
    "\n",
    "**1. Begin with baselines for classification.** Your target to predict is `shot_made_flag`. What is your baseline accuracy, if you guessed the majority class for every prediction?\n",
    "\n",
    "**2. Hold out your test set.** Use the 2018-19 season to test. NBA seasons begin in October and end in June. You'll know you've split the data correctly when your test set has 1,709 observations.\n",
    "\n",
    "**3. Engineer new feature.** Engineer at least **1** new feature, from this list, or your own idea.\n",
    "- **Homecourt Advantage**: Is the home team (`htm`) the Golden State Warriors (`GSW`) ?\n",
    "- **Opponent**: Who is the other team playing the Golden State Warriors?\n",
    "- **Seconds remaining in the period**: Combine minutes remaining with seconds remaining, to get the total number of seconds remaining in the period.\n",
    "- **Seconds remaining in the game**: Combine period, and seconds remaining in the period, to get the total number of seconds remaining in the game. A basketball game has 4 periods, each 12 minutes long.\n",
    "- **Made previous shot**: Was Steph Curry's previous shot successful?\n",
    "\n",
    "**4. Decide how to validate** your model. Choose one of the following options. Any of these options are good. You are not graded on which you choose.\n",
    "- **Train/validate/test split:Â train on the 2009-10 season through 2016-17 season, validate with the 2017-18 season.** You'll know you've split the data correctly when your train set has 11,081 observations, and your validation set has 1,168 observations.\n",
    "- **Train/validate/test split:Â random 80/20%** train/validate split.\n",
    "- **Cross-validation** with independent test set. You may use any scikit-learn cross-validation method.\n",
    "\n",
    "**5.** Use a scikit-learn **pipeline** to **encode categoricals** and fit a **Decision Tree** or **Random Forest** model.\n",
    "\n",
    "**6.** Get your model's **validation accuracy.** (Multiple times if you try multiple iterations.) \n",
    "\n",
    "**7.** Get your model's **test accuracy.** (One time, at the end.)\n",
    "\n",
    "\n",
    "**8.** Given a **confusion matrix** for a hypothetical binary classification model, **calculate accuracy, precision, and recall.**\n",
    "\n",
    "### Stretch Goals\n",
    "- Engineer 4+ new features total, either from the list above, or your own ideas.\n",
    "- Make 2+ visualizations to explore relationships between features and target.\n",
    "- Optimize 3+ hyperparameters by trying 10+ \"candidates\" (possible combinations of hyperparameters). You can use `RandomizedSearchCV` or do it manually.\n",
    "- Get and plot your model's feature importances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Pandas Profiling version\n",
    "import pandas_profiling\n",
    "pandas_profiling.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(df, minimal=True)\n",
    "\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6Jt3qjQ-zig"
   },
   "source": [
    "## 1. Begin with baselines for classification. \n",
    "\n",
    ">Your target to predict is `shot_made_flag`. What would your baseline accuracy be, if you guessed the majority class for every prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0BDeNFG_Kee"
   },
   "outputs": [],
   "source": [
    "y_val = df['shot_made_flag']\n",
    "y_val.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority = y_val.mode()[0]\n",
    "y_pred = [majority]*len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dz2QHBiVy_Br"
   },
   "source": [
    "## 2. Hold out your test set.\n",
    "\n",
    ">Use the 2018-19 season to test. NBA seasons begin in October and end in June. You'll know you've split the data correctly when your test set has 1,709 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['game_date'] = pd.to_datetime(df['game_date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OPod6lBG_wTT"
   },
   "outputs": [],
   "source": [
    "#train, val, test\n",
    "cutoff = pd.to_datetime('2018-07-01')\n",
    "train = df[df['game_date'] < cutoff]\n",
    "test  = df[df['game_date'] >= cutoff]\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9Nihzk6y_CF"
   },
   "source": [
    "## 3. Engineer new feature.\n",
    "\n",
    ">Engineer at least **1** new feature, from this list, or your own idea.\n",
    ">\n",
    ">- **Homecourt Advantage**: Is the home team (`htm`) the Golden State Warriors (`GSW`) ?\n",
    ">- **Opponent**: Who is the other team playing the Golden State Warriors?\n",
    ">- **Seconds remaining in the period**: Combine minutes remaining with seconds remaining, to get the total number of seconds remaining in the period.\n",
    ">- **Seconds remaining in the game**: Combine period, and seconds remaining in the period, to get the total number of seconds remaining in the game. A basketball game has 4 periods, each 12 minutes long.\n",
    ">- **Made previous shot**: Was Steph Curry's previous shot successful?\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0pxdFtWy_Bz"
   },
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # homecourt advantage\n",
    "    df['home_advantage'] = (df['htm'] == 'GSW')\n",
    "    \n",
    "    # Opponent\n",
    "    # df['opponent'] = (df['htm'] != 'GSW') | (df['vtm'] != 'GSW')\n",
    "    # this didn't extract the value.\n",
    "    \n",
    "    # Seconds remaining in period \n",
    "    df['seconds_remaining_in_period'] = (df['minutes_remaining']*60) +(df['seconds_remaining'])\n",
    "    \n",
    "    # Drop columns that were used for engineering\n",
    "    # df = df.drop(columns='htm')\n",
    "    # df = df.drop(columns='vtm')\n",
    "    df = df.drop(columns='minutes_remaining')\n",
    "    df = df.drop(columns='seconds_remaining')\n",
    "    df = df.drop(columns='game_date')\n",
    "    df = df.drop(columns='game_id')\n",
    "    df = df.drop(columns='game_event_id')\n",
    "    df = df.drop(columns='player_name')\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = engineer_features(train)\n",
    "test = engineer_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLs7pt7NFJLF"
   },
   "source": [
    "## **4. Decide how to validate** your model. \n",
    "\n",
    ">Choose one of the following options. Any of these options are good. You are not graded on which you choose.\n",
    ">\n",
    ">- **Train/validate/test split:Â train on the 2009-10 season through 2016-17 season, validate with the 2017-18 season.** You'll know you've split the data correctly when your train set has 11,081 observations, and your validation set has 1,168 observations.\n",
    ">- **Train/validate/test split:Â random 80/20%** train/validate split.\n",
    ">- **Cross-validation** with independent test set. You may use any scikit-learn cross-validation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJ58CceDISXR"
   },
   "outputs": [],
   "source": [
    "target = 'shot_made_flag'\n",
    "features = train.columns.drop([target])\n",
    "X_train = train[features]\n",
    "y_train = train[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQ2lWlu7JPRt"
   },
   "source": [
    "## 5. Use a scikit-learn pipeline to encode categoricals and fit a Decision Tree or Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    RandomForestClassifier(n_jobs=-1, random_state=7, max_depth=20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2S8mUuJy_CB"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_distributions = {\n",
    "    'simpleimputer__strategy':['mean'],\n",
    "    'randomforestclassifier__max_depth':[5, 10, 15, 20, None],\n",
    "    'randomforestclassifier__n_estimators': randint(50,500)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=15,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    verbose=10,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters', search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(search.cv_results_).sort_values(by='rank_test_score').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8kJXxFpty_CH"
   },
   "source": [
    "## 6.Get your model's validation accuracy\n",
    "\n",
    "> (Multiple times if you try multiple iterations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YvyYY9tfy_CL"
   },
   "source": [
    "## 7. Get your model's test accuracy\n",
    "\n",
    "> (One time, at the end.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy:', -search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjV2dfl6y_CL"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[target]\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Test MAE: {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xGL5stLvJCn1"
   },
   "source": [
    "## 8. Given a confusion matrix, calculate accuracy, precision, and recall.\n",
    "\n",
    "Imagine this is the confusion matrix for a binary classification model. Use the confusion matrix to calculate the model's accuracy, precision, and recall.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td colspan=\"2\" rowspan=\"2\"></td>\n",
    "    <td colspan=\"2\">Predicted</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Negative</td>\n",
    "    <td>Positive</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Actual</td>\n",
    "    <td>Negative</td>\n",
    "    <td style=\"border: solid\">85</td>\n",
    "    <td style=\"border: solid\">58</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Positive</td>\n",
    "    <td style=\"border: solid\">8</td>\n",
    "    <td style=\"border: solid\"> 36</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nEvt7NkUJNao"
   },
   "source": [
    "### Calculate accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFszS2A5JJmv"
   },
   "outputs": [],
   "source": [
    "#from scikitplot.metrics import plot_confusion_matrix\n",
    "#plot_confusion_matrix(y_test, y_pred, figsize=(20,10),title=f\"Confusion Matric: N={len(y_test)}\",normalize=False);\n",
    "# I was using the wrong data for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (85+36)/(85+58+8+36)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjHTmk8sJO4v"
   },
   "source": [
    "### Calculate precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_predict = 85+8\n",
    "pos_predict = 58+36\n",
    "correct_pos = 36\n",
    "correct_neg = 85\n",
    "\n",
    "neg_precision = correct_neg / neg_predict\n",
    "pos_precision = correct_pos / pos_predict\n",
    "\n",
    "print('Precision for negative predictions:', neg_precision)\n",
    "print('Precision for positive predictions:', pos_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFug3ZKaJQ7A"
   },
   "source": [
    "### Calculate recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qX1gbcMJQS_"
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import classification_report\n",
    "#print(classification_report(y_test, y_pred))\n",
    "# I was using the wrong data for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L0OKc3JxJR4r"
   },
   "outputs": [],
   "source": [
    "neg_actual = 85+58\n",
    "pos_actual = 8+36\n",
    "\n",
    "neg_recall = correct_neg / neg_actual\n",
    "pos_recall = correct_pos / pos_actual\n",
    "\n",
    "print('Recall for negative predictions:', neg_recall)\n",
    "print('Recall for positive predictions:', pos_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS_Sprint_Challenge_6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
